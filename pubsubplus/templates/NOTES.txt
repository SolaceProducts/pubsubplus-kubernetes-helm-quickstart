== Your Solace PubSub+ deployment is in progress now ==

Watch progress by running 'kubectl get pods --show-labels -w'

Deployment is complete when a Solace pod representing an active event broker node's label reports "active=true".

For troubleshooting, refer to ***TroubleShooting.md***

**Admin access and credentials**
{{- if not .Values.solace.usernameAdminPassword }}
*******************************************************************
* Important:
* An admin password has been auto-generated. You must retrieve it
* and provide it as value override if using Helm upgrade
* otherwise your cluster will become unusable.
*******************************************************************

{{- end }}
    Username      : admin
    Password      : echo `kubectl get secret {{ template "solace.fullname" . }}-secrets -o jsonpath="{.data.username_admin_password}" | base64 --decode`

**Image used**
{{ .Values.image.repository }}:{{ .Values.image.tag }}

**Storage used**

**Performance**
{{- if contains "dev" .Values.solace.size }}
This is a minimum footprint deployment for development purposes without any guaranteed performance.
{{- else }}
The connection scaling tier for this deployment is: max {{ substr 4 10 .Values.solace.size }} connections.
{{- end }}

**Access**
PubSub+ can be accessed within the cluster on port {{ .Values.service.nodePort }} at {{ template "solace.fullname" . }}.{{ .Release.Namespace }}.svc.cluster.local



To access for outside the cluster, perform the following steps:

{{- if contains "NodePort" .Values.service.type }}

Obtain the NodePort IP and ports:

    export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath="{.items[0].status.addresses[0].address}")
    export NODE_PORT_AMQP=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath="{.spec.ports[1].nodePort}" services {{ template "solace.fullname" . }})
    export NODE_PORT_STATS=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath="{.spec.ports[3].nodePort}" services {{ template "solace.fullname" . }})

To Access the PubSub+ AMQP port:

    echo "URL : amqp://$NODE_IP:$NODE_PORT_AMQP/"

To Access the PubSub+ Management interface:

    echo "URL : http://$NODE_IP:$NODE_PORT_STATS/"

{{- else if contains "LoadBalancer" .Values.service.type }}
The k8s service type is LoadBalancer.

Obtain the LoadBalancer IP and the service addresses:

NOTE: It may take a few minutes for the LoadBalancer IP to be available.
      Watch the status with: 'kubectl get svc --namespace {{ .Release.Namespace }} -w {{ template "solace.fullname" . }}'

    export SERVICE_IP=$(kubectl get svc --namespace {{ .Release.Namespace }} {{ template "solace.fullname" . }} --template "{{"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}"}}")
    echo -e "Protocol\tAddress\n"`kubectl get svc --namespace {{ .Release.Namespace }} {{ template "solace.fullname" . }} -o jsonpath="{range .spec.ports[*]}{.name}\t$SERVICE_IP:{.port}\n"`

{{- else if contains "ClusterIP"  .Values.service.type }}
The k8s service type is ClusterIP.

To access services from pods within the k8s cluster, use these addresses:

    echo -e "Protocol\tAddress\n"`kubectl get svc --namespace {{ .Release.Namespace }} {{ template "solace.fullname" . }} -o jsonpath="{range .spec.ports[*]}{.name}\t{{ template "solace.fullname" . }}.{{ .Release.Namespace }}.svc.cluster.local:{.port}\n"`

For local testing purposes you can also use port-forward in a background process to map ports to local host, then use these service addresses:

    kubectl port-forward --namespace {{ .Release.Namespace }} svc/{{ template "solace.fullname" . }} $(echo `kubectl get svc --namespace {{ .Release.Namespace }} {{ template "solace.fullname" . }} -o jsonpath="{range .spec.ports[*]}{.targetPort}:{.port} "`) &
    echo -e "Protocol\tAddress\n"`kubectl get svc --namespace {{ .Release.Namespace }} {{ template "solace.fullname" . }} -o jsonpath="{range .spec.ports[*]}{.name}\t127.0.0.1:{.targetPort}\n"`

{{- end }}
